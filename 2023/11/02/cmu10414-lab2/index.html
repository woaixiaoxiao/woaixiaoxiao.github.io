

<!DOCTYPE html>
<html lang="zh-CN" data-default-color-scheme=light>



<head>
  <meta charset="UTF-8">
  <meta name="referrer" content="no-referrer">
  <link rel="stylesheet" href="https://npm.elemecdn.com/lxgw-wenkai-screen-webfont/style.css" media="print" onload="this.media='all'">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/fluid.png">
  <link rel="icon" href="/img/fluid.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="Cao Dong">
  <meta name="keywords" content="">
  
    <meta name="description" content="Introduction to needleQuestion 1这部分是完成一些初始化的操作，我们会用到init_basic中已经准备好的函数，这部分主要是对numpy的操作进行了一些封装，可以指定设备为cpu还是gpu，可以指定数据类型，可以指定是否需要梯度，并且返回的是Tensor xavier_uniform根据公式来就行，注意传参的方式，因为给我们提供的函数签名是这样rand(*shape">
<meta property="og:type" content="article">
<meta property="og:title" content="cmu10414_lab2">
<meta property="og:url" content="http://example.com/2023/11/02/cmu10414-lab2/index.html">
<meta property="og:site_name" content="dcao&#39;s blog">
<meta property="og:description" content="Introduction to needleQuestion 1这部分是完成一些初始化的操作，我们会用到init_basic中已经准备好的函数，这部分主要是对numpy的操作进行了一些封装，可以指定设备为cpu还是gpu，可以指定数据类型，可以指定是否需要梯度，并且返回的是Tensor xavier_uniform根据公式来就行，注意传参的方式，因为给我们提供的函数签名是这样rand(*shape">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://woaixiaoxiao-image.oss-cn-beijing.aliyuncs.com/img/image-20231031200321862.png">
<meta property="article:published_time" content="2023-11-01T20:28:08.000Z">
<meta property="article:modified_time" content="2023-12-31T08:23:22.803Z">
<meta property="article:author" content="Cao Dong">
<meta property="article:tag" content="深度学习框架">
<meta property="article:tag" content="CMU10414">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://woaixiaoxiao-image.oss-cn-beijing.aliyuncs.com/img/image-20231031200321862.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>cmu10414_lab2 - dcao&#39;s blog</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"example.com","root":"/","version":"1.9.7","typing":{"enable":true,"typeSpeed":70,"cursorChar":"_","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":{"measurement_id":null},"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml","include_content_in_search":true};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<meta name="generator" content="Hexo 7.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 70vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>CaoDong</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/" target="_self">
                <i class="iconfont icon-home-fill"></i>
                <span>首页</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/" target="_self">
                <i class="iconfont icon-archive-fill"></i>
                <span>归档</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/" target="_self">
                <i class="iconfont icon-category-fill"></i>
                <span>分类</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/" target="_self">
                <i class="iconfont icon-tags-fill"></i>
                <span>标签</span>
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/" target="_self">
                <i class="iconfont icon-user-fill"></i>
                <span>关于</span>
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              <i class="iconfont icon-search"></i>
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">
              <i class="iconfont icon-dark" id="color-toggle-icon"></i>
            </a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/scenery/p1.png') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="cmu10414_lab2"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Cao Dong
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2023-11-02 04:28" pubdate>
          2023年11月2日 凌晨
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          4.5k 字
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          38 分钟
        
      </span>
    

    
    
      
        <span id="busuanzi_container_page_pv" style="display: none">
          <i class="iconfont icon-eye" aria-hidden="true"></i>
          <span id="busuanzi_value_page_pv"></span> 次
        </span>
        
      
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <h1 id="seo-header">cmu10414_lab2</h1>
            
            
              <div class="markdown-body">
                
                <h1 id="Introduction-to-needle"><a href="#Introduction-to-needle" class="headerlink" title="Introduction to needle"></a>Introduction to <code>needle</code></h1><h1 id="Question-1"><a href="#Question-1" class="headerlink" title="Question 1"></a>Question 1</h1><p>这部分是完成一些初始化的操作，我们会用到<code>init_basic</code>中已经准备好的函数，这部分主要是对numpy的操作进行了一些封装，可以指定设备为cpu还是gpu，可以指定数据类型，可以指定是否需要梯度，并且返回的是Tensor</p>
<h2 id="xavier-uniform"><a href="#xavier-uniform" class="headerlink" title="xavier_uniform"></a>xavier_uniform</h2><p>根据公式来就行，注意传参的方式，因为给我们提供的函数签名是这样<code>rand(*shape, low=0.0, high=1.0, device=None, dtype=&quot;float32&quot;, requires_grad=False)</code>，<code>*s</code>代表我们可以传入很多个变量，都会归到s中，正确的调用方式如下</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">xavier_uniform</span><span class="hljs-params">(fan_in, fan_out, gain=<span class="hljs-number">1.0</span>, kwargs)</span>:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    a =</span> gain * math.<span class="hljs-built_in">sqrt</span>(<span class="hljs-number">6</span> / (fan_in + fan_out))<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;hello  &quot;</span>, a)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">rand</span>(fan_in, fan_out, low=-a, high=a, kwargs)<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<p>我一开始使用<code>rand((fan_in, fan_out), -a, a, kwargs)</code>，这样是错误的，Python不会自动将元组翻译为s，只能一个参数一个参数地传。</p>
<p>而因为rand函数第一个参数就是<code>*s</code>，导致后面就没法正常传入-a和a，必须制定<code>low=-a,high=a</code>，至于最后一个参数代表的是多个字典，因此直接<code>kwargs</code>传入即可</p>
<h2 id="others"><a href="#others" class="headerlink" title="others"></a>others</h2><p>通过第一个熟悉语法基础之后，其他的就是翻译一下公式</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">xavier_normal</span><span class="hljs-params">(fan_in, fan_out, gain=<span class="hljs-number">1.0</span>, kwargs)</span>:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    std =</span> gain * math.<span class="hljs-built_in">sqrt</span>(<span class="hljs-number">2</span> / (fan_in + fan_out))<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">randn</span>(fan_in, fan_out, std=std, kwargs)<br>    # END YOUR SOLUTION<br><br><br>def <span class="hljs-built_in">kaiming_uniform</span>(fan_in, fan_out, nonlinearity=<span class="hljs-string">&quot;relu&quot;</span>, kwargs):<br>    assert nonlinearity == <span class="hljs-string">&quot;relu&quot;</span>, <span class="hljs-string">&quot;Only relu supported currently&quot;</span><br>    # BEGIN YOUR SOLUTION<br>    bound = math.<span class="hljs-built_in">sqrt</span>(<span class="hljs-number">2</span>) * math.<span class="hljs-built_in">sqrt</span>(<span class="hljs-number">3</span> / fan_in)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">rand</span>(fan_in, fan_out, low=-bound, high=bound, kwargs)<br>    # END YOUR SOLUTION<br><br><br>def <span class="hljs-built_in">kaiming_normal</span>(fan_in, fan_out, nonlinearity=<span class="hljs-string">&quot;relu&quot;</span>, kwargs):<br>    assert nonlinearity == <span class="hljs-string">&quot;relu&quot;</span>, <span class="hljs-string">&quot;Only relu supported currently&quot;</span><br>    # BEGIN YOUR SOLUTION<br>    std = math.<span class="hljs-built_in">sqrt</span>(<span class="hljs-number">2</span>) * math.<span class="hljs-built_in">sqrt</span>(<span class="hljs-number">1</span> / fan_in)<br>    <span class="hljs-keyword">return</span> <span class="hljs-built_in">randn</span>(fan_in, fan_out, std=std, kwargs)<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<h1 id="Question-2"><a href="#Question-2" class="headerlink" title="Question 2"></a>Question 2</h1><h2 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h2><h3 id="init"><a href="#init" class="headerlink" title="init"></a>init</h3><p>直接调用之前写好的初始化函数即可，这里要手动传入device和dtype等信息</p>
<p>关键是bias函数需要自己手动reshape一下</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs c++">self.weight = init.<span class="hljs-built_in">kaiming_uniform</span>(<br>    in_features, out_features, device=device, dtype=dtype)<br>self.bias = init.<span class="hljs-built_in">kaiming_uniform</span>(<br>    out_features, <span class="hljs-number">1</span>, device=device, dtype=dtype).<span class="hljs-built_in">reshape</span>((<span class="hljs-number">1</span>, out_features))<br><br></code></pre></td></tr></table></figure>

<h3 id="forward"><a href="#forward" class="headerlink" title="forward"></a>forward</h3><p>直接翻译公式即可</p>
<p>关键是要手动将bias广播成最终的shape，可以在乘法结束之后取出shape，这样省的自己拼拼凑凑了</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">forward</span><span class="hljs-params">(self, X: Tensor)</span> -&gt; Tensor:</span><br><span class="hljs-function"># BEGIN YOUR SOLUTION</span><br><span class="hljs-function">	xat =</span> X.<span class="hljs-built_in">matmul</span>(self.weight)<br>    shape = xat.shape<br>    bb = self.bias.<span class="hljs-built_in">broadcast_to</span>(shape)<br>    <span class="hljs-keyword">return</span> ops.<span class="hljs-built_in">add</span>(xat, bb)<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<h2 id="relu"><a href="#relu" class="headerlink" title="relu"></a>relu</h2><h3 id="forward-1"><a href="#forward-1" class="headerlink" title="forward"></a>forward</h3><p>这里好像是说之后反向传播时，直接将relu在0处的导数看为0</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">forward</span><span class="hljs-params">(self, x: Tensor)</span> -&gt; Tensor:</span><br><span class="hljs-function"># BEGIN YOUR SOLUTION</span><br><span class="hljs-function">	return ops.relu(x)</span><br><span class="hljs-function">    # END YOUR SOLUTION</span><br></code></pre></td></tr></table></figure>

<h2 id="sequential"><a href="#sequential" class="headerlink" title="sequential"></a>sequential</h2><h3 id="forward-2"><a href="#forward-2" class="headerlink" title="forward"></a>forward</h3><p>定义了一个批量化操作，到时候传入一堆模块，就可以按顺序执行这些模块了</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">forward</span><span class="hljs-params">(self, x: Tensor)</span> -&gt; Tensor:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    c =</span> x<br>    <span class="hljs-keyword">for</span> m in self.modules:<br>        c = <span class="hljs-built_in">m</span>(c)<br>    <span class="hljs-keyword">return</span> c<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<h2 id="LogSumExp"><a href="#LogSumExp" class="headerlink" title="LogSumExp"></a>LogSumExp</h2><p>这个算子就有点复杂了，特别是在反向传播的时候</p>
<p>不过反向传播的求导公式化简之后，普通的$z_i$和$maxz$的表达式是一样的，即<br>$$<br>\frac{e^{z_i-maxz}}{ze^{z_i-maxz}}<br>$$</p>
<h3 id="forward-3"><a href="#forward-3" class="headerlink" title="forward"></a>forward</h3><p>前向传播只需要翻译公式，和hw1里的方法差不多，就是要注意维度的问题</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">compute</span><span class="hljs-params">(self, Z)</span>:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    maxZ =</span> array_api.<span class="hljs-built_in">amax</span>(Z, axis=self.axes, keepdims=True)<br>    z_exp_minus = array_api.<span class="hljs-built_in">exp</span>(Z - maxZ)<br>    z_sum = array_api.<span class="hljs-built_in">sum</span>(z_exp_minus, axis=self.axes)<br>    z_log = array_api.<span class="hljs-built_in">log</span>(z_sum)<br>    z_ans = z_log + maxZ.<span class="hljs-built_in">reshape</span>(z_log.shape)<br>    <span class="hljs-keyword">return</span> z_ans<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<h3 id="backward"><a href="#backward" class="headerlink" title="backward"></a>backward</h3><p>求导公式在前面已经给出</p>
<ol>
<li><p>我们在backward的时候，要使用tensor，不能使用numpy的数组</p>
</li>
<li><p>而目前还没有实现tensor的max操作，所以需要先将数据提取出来进行max，之后将maxz作为一个标量和tensor操作一下，结果就是tensor了</p>
</li>
</ol>
<p>在这里我突然有点不理解为什么forward的时候，为什么返回的numpy数组，却可以生成tensor。调试了一下发现，我忘记了这些操作的类都是从TensorOp中继承的，而这个类将它的<code>__call__</code>方法设置为<code>make_from_op</code>。而在<code>make_from_op</code>中，它先初始化一个Tensor，然后通过<code>realize_cached_data</code>调用了<code>compute</code>方法，才真正的给这个tensor赋值。也就是说，forward只是构建一个tensor里的一小步，这里面提供了很多封装。</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">gradient</span><span class="hljs-params">(self, out_grad, node)</span>:</span><br><span class="hljs-function">    z =</span> node.inputs[<span class="hljs-number">0</span>]<br>    maxz = z.<span class="hljs-built_in">realize_cached_data</span>().<span class="hljs-built_in">max</span>(self.axes, keepdims=True)<br>    zexp = <span class="hljs-built_in">exp</span>(z - maxz)<br>    zsumexp = <span class="hljs-built_in">summation</span>(zexp, self.axes)<br>    grad_div_zse = out_grad / zsumexp<br>    grad_div_zse_b = grad_div_zse.<span class="hljs-built_in">reshape</span>(maxz.shape).<span class="hljs-built_in">broadcast_to</span>(z.shape)<br>    <span class="hljs-keyword">return</span> grad_div_zse_b * zexp<br></code></pre></td></tr></table></figure>

<p>这里的坑点就是不能使用array_api的函数，要使用自己写的继承自TensorOp的函数，这样才可以生成并返回Tensor</p>
<h2 id="SoftmaxLoss"><a href="#SoftmaxLoss" class="headerlink" title="SoftmaxLoss"></a>SoftmaxLoss</h2><h3 id="forward-4"><a href="#forward-4" class="headerlink" title="forward"></a>forward</h3><p>logits：$[m,k]$，y：$[m,]$</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-keyword">class</span> <span class="hljs-title">SoftmaxLoss</span><span class="hljs-params">(Module)</span>:</span><br><span class="hljs-function">    def forward(self, logits: Tensor, y: Tensor):</span><br><span class="hljs-function">        # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">        pre =</span> ops.<span class="hljs-built_in">summation</span>(ops.<span class="hljs-built_in">logsumexp</span>(logits, axes=(<span class="hljs-number">1</span>)))<br>        oh = init.<span class="hljs-built_in">one_hot</span>(logits.shape[<span class="hljs-number">1</span>], y)<br>        lat = ops.<span class="hljs-built_in">summation</span>(ops.<span class="hljs-built_in">multiply</span>(oh, logits))<br>        <span class="hljs-built_in">return</span> (pre - lat) / logits.shape[<span class="hljs-number">0</span>]<br>        # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<ol>
<li>调用之前写的加强版的logsumexp计算减号前面的之和</li>
<li>调用init.one_hot生成one_hot矩阵，这里传入的y需要是tensor，因为在one_hot函数中，调用了y.numpy</li>
<li>通过逐元素相乘然后求和，得到减号后面的</li>
<li>最后记得除以样本数量，得到平均值</li>
</ol>
<h2 id="LayerNorm1d"><a href="#LayerNorm1d" class="headerlink" title="LayerNorm1d"></a>LayerNorm1d</h2><p>翻译公式即可，不过这里面形状的变化有点复杂</p>
<p>首先，我们的参数是需要自己自定义的，观察公式就可以知道，weight和bias就是一个形状为$(n,)$的向量，初始化时一个为全1，一个为全0</p>
<p>其次，我们的输入一定是一个二维的tensor，因此，在后面各种操作时，我们可以轻松将tensor给reshape成想要的形状，注意点</p>
<ol>
<li>tensor是没有自动的广播功能的，因此，我们最好在操作之前，手动广播维度</li>
<li>低维的矩阵，比如形状为$[a,b]$，可以直接广播成$[c,a,b]$</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-keyword">class</span> <span class="hljs-title">LayerNorm1d</span><span class="hljs-params">(Module)</span>:</span><br><span class="hljs-function">    def __init__(self, dim, eps=</span><span class="hljs-number">1e-5</span>, device=None, dtype=<span class="hljs-string">&quot;float32&quot;</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.dim = dim<br>        self.eps = eps<br>        # BEGIN YOUR SOLUTION<br>        <span class="hljs-meta"># self.weight = Parameter(init.ones(dim, requires_grad=True))</span><br>        self.weight = init.<span class="hljs-built_in">ones</span>(dim)<br>        self.bias = init.<span class="hljs-built_in">zeros</span>(dim)<br>        # END YOUR SOLUTION<br><br>    def forward(self, x: Tensor) -&gt; Tensor:<br>        # BEGIN YOUR SOLUTION<br>        mean = (x.<span class="hljs-built_in">sum</span>((<span class="hljs-number">1</span>,)) /<br>                x.shape[<span class="hljs-number">1</span>]).<span class="hljs-built_in">reshape</span>((x.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)).<span class="hljs-built_in">broadcast_to</span>(x.shape)<br>        var = (((x - mean)<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>((<span class="hljs-number">1</span>,)) /<br>               x.shape[<span class="hljs-number">1</span>]).<span class="hljs-built_in">reshape</span>((x.shape[<span class="hljs-number">0</span>], <span class="hljs-number">1</span>)).<span class="hljs-built_in">broadcast_to</span>(x.shape)<br>        deno = (var + self.eps)<span class="hljs-number">0.5</span><br>        <span class="hljs-keyword">return</span> self.weight.<span class="hljs-built_in">broadcast_to</span>(x.shape) * (x - mean) / deno + self.bias.<span class="hljs-built_in">broadcast_to</span>(x.shape)<br>        # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<h2 id="Flatten"><a href="#Flatten" class="headerlink" title="Flatten"></a>Flatten</h2><p>这个简单，计算出维度之后直接reshape就行</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function"><span class="hljs-keyword">class</span> <span class="hljs-title">Flatten</span><span class="hljs-params">(Module)</span>:</span><br><span class="hljs-function">    def forward(self, X):</span><br><span class="hljs-function">        # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">        s =</span> X.shape<br>        x = s[<span class="hljs-number">0</span>]<br>        y = <span class="hljs-number">1</span><br>        <span class="hljs-keyword">for</span> i in s[<span class="hljs-number">1</span>:]:<br>            y *= i<br>        <span class="hljs-keyword">return</span> X.<span class="hljs-built_in">reshape</span>((x, y))<br>        # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<h2 id="BatchNorm1d"><a href="#BatchNorm1d" class="headerlink" title="BatchNorm1d"></a>BatchNorm1d</h2><p>这个归一化的操作很神奇，是对在同一个特征位置上的值进行归一化，而不是在同一个样本内部进行归一化，因此像sum这种操作的轴都是0，也就是列</p>
<p>并且可以在训练的时候不断地学习，最后在测试的时候使用统计好的数据</p>
<p>这里给的公式有一点歧义</p>
<p><img src="http://woaixiaoxiao-image.oss-cn-beijing.aliyuncs.com/img/image-20231031200321862.png" srcset="/img/loading.gif" lazyload alt="image-20231031200321862"></p>
<p>上面是训练的，没啥问题，下面是测试的，这里直接就是y等于这一堆，这就有点问题，这只是对数据进行了归一化操作，因此，还需要乘w加b之后才对。并且上面的$mu$其实就是之前统计的平均数，下方的theta的平方，就是统计好的方差。</p>
<p>最后，这里要用$self.training$来判断当前是在训练还是在测试</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">forward</span><span class="hljs-params">(self, x: Tensor)</span> -&gt; Tensor:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    if self.training:</span><br><span class="hljs-function">        # ex =</span> (x.<span class="hljs-built_in">sum</span>((<span class="hljs-number">0</span>,)) / x.shape[<span class="hljs-number">0</span>]).<span class="hljs-built_in">broadcast_to</span>(x.shape)<br>        <span class="hljs-meta"># vx = (((x - ex)2).sum((0,)) /</span><br>        <span class="hljs-meta">#       x.shape[0]).broadcast_to(x.shape)</span><br>        ex = (x.<span class="hljs-built_in">sum</span>((<span class="hljs-number">0</span>,)) / x.shape[<span class="hljs-number">0</span>])<br>        vx = (((x - ex.<span class="hljs-built_in">broadcast_to</span>(x.shape))<span class="hljs-number">2</span>).<span class="hljs-built_in">sum</span>((<span class="hljs-number">0</span>,)) / x.shape[<span class="hljs-number">0</span>])<br>        self.running_mean = (<span class="hljs-number">1</span> - self.momentum) * self.running_mean + \<br>            self.momentum * ex<br>        self.running_var = (<span class="hljs-number">1</span> - self.momentum) * \<br>            self.running_var + self.momentum * vx<br>        norm = (x - ex.<span class="hljs-built_in">broadcast_to</span>(x.shape)) / \<br>            ((vx.<span class="hljs-built_in">broadcast_to</span>(x.shape) + self.eps)<span class="hljs-number">0.5</span>)<br>        <span class="hljs-keyword">return</span> self.weight.<span class="hljs-built_in">broadcast_to</span>(x.shape) * norm + self.bias.<span class="hljs-built_in">broadcast_to</span>(x.shape)<br>    <span class="hljs-keyword">else</span>:<br>        norm = (x - self.running_mean.<span class="hljs-built_in">broadcast_to</span>(x.shape)) / (<br>            (self.running_var.<span class="hljs-built_in">broadcast_to</span>(x.shape) + self.eps)(<span class="hljs-number">0.5</span>))<br>        <span class="hljs-keyword">return</span> self.weight.<span class="hljs-built_in">broadcast_to</span>(x.shape) * norm + self.bias.<span class="hljs-built_in">broadcast_to</span>(x.shape)<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<p>注意坑</p>
<ol>
<li>要用到axis的操作，最好是用元组传递参数，不要用整数，比如sum操作。因为我们在这里默认是认为axis可以迭代，即可以for循环访问的，不像numpy原生的操作</li>
</ol>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h2><p>dropout的实现就是生成一个二进制数组，其中1的比例就是p</p>
<p>框架已经给我们提供了一种初始化方法<code>randb</code>，直接调用就行了，唯一的问题是题目传入的p代表的是0的比例，因此用1-p作为参数传入</p>
<p>最后要记得将数组放大，这样才能保证均值和方差不变</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">forward</span><span class="hljs-params">(self, x: Tensor)</span> -&gt; Tensor:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    if self.training:</span><br><span class="hljs-function">        mask =</span> init.<span class="hljs-built_in">randb</span>(*x.shape, p=<span class="hljs-number">1</span> - self.p)<br>        <span class="hljs-keyword">return</span> x * mask / (<span class="hljs-number">1</span> - self.p)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">return</span> x<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<h2 id="Residual"><a href="#Residual" class="headerlink" title="Residual"></a>Residual</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">forward</span><span class="hljs-params">(self, x: Tensor)</span> -&gt; Tensor:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    return self.fn(x) + x</span><br><span class="hljs-function">    # END YOUR SOLUTION</span><br></code></pre></td></tr></table></figure>

<h1 id="Question-3"><a href="#Question-3" class="headerlink" title="Question 3"></a>Question 3</h1><h2 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h2><p>主体部分翻译公式即可，不过这里要注意</p>
<ol>
<li><p><code>self.u</code>是一个字典，以parameter为键，以对应的被动量修正过的梯度为值</p>
</li>
<li><p>这里只会更新Parameter类型的变量，所以之前各种模块里的要训练的参数都要声明为parameter，并且梯度需要是可更新的，就像这样</p>
 <figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs c++">self.weight = <span class="hljs-built_in">Parameter</span>(init.<span class="hljs-built_in">ones</span>(dim, device=device, dtype=dtype, requires_grad=True))<br></code></pre></td></tr></table></figure>
</li>
<li><p>这里多给了一个<code>weight_decay</code>，这个参数是用来正则化的，使用的是L2范数。具体在使用上，使用它和矩阵的变量相乘（在损失函数中是$1&#x2F;2*w^2$，求导后就是$w$，这个weight_decay应该相当于一个系数。有点问题的就是，这里应该要用w的绝对值才准确，但是我没加绝对值也过了），然后加上梯度上即可。</p>
</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">step</span><span class="hljs-params">(self)</span>:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    for parm in self.params:</span><br><span class="hljs-function">        if self.weight_decay &gt; <span class="hljs-number">0</span>:</span><br><span class="hljs-function">            grad =</span> parm.grad.data + self.weight_decay * parm.data<br>        <span class="hljs-keyword">else</span>:<br>            grad = parm.grad.data<br>        self.u[parm] = self.momentum * \<br>            self.u[parm] + (<span class="hljs-number">1</span> - self.momentum) * grad<br>        parm.data = parm.data - self.lr * self.u[parm]<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<p>最后这里有个奇怪的点就是，我这里一直报错显示有一部分数据是float64，在<code>parm.data = parm.data - self.lr * self.u[parm]</code>类型检查一直出错。后来检查到是<code>SoftmaxLoss</code>模块的问题，在这里计算loss时，我是先计算减号前的和，以及减号后的和，相减之后再除以样本数量，这样就会出错。而如果在求和之前先除样本数就不会错。问了gpt之后说是Python在溢出的时候会默认提升位数，估计是直接加起来太大了，导致从float32变成float64</p>
<h1 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h1><p>依旧是翻译公式，有以下注意点</p>
<ol>
<li><code>selr.t</code>每调用一次step就+1，目前还不是很懂这个原理是啥，应该是用来调节每次的梯度对u和v的影响程度的</li>
</ol>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">step</span><span class="hljs-params">(self)</span>:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    self.t +=</span> <span class="hljs-number">1</span><br>    <span class="hljs-keyword">for</span> parm in self.params:<br>        <span class="hljs-keyword">if</span> self.weight_decay &gt; <span class="hljs-number">0</span>:<br>            grad = parm.grad.data + self.weight_decay * parm.data<br>        <span class="hljs-keyword">else</span>:<br>            grad = parm.grad.data<br>        self.m[parm] = self.beta1 * self.m[parm] + (<span class="hljs-number">1</span> - self.beta1) * grad<br>        self.v[parm] = self.beta2 * self.v[parm] + \<br>            (<span class="hljs-number">1</span> - self.beta2) * (grad * grad)<br>        bm = self.m[parm] / (<span class="hljs-number">1</span> - self.beta1self.t)<br>        bv = self.v[parm] / (<span class="hljs-number">1</span> - self.beta2self.t)<br>        parm.data = parm.data - self.lr * bm / (<span class="hljs-built_in">bv</span>(<span class="hljs-number">1</span> / <span class="hljs-number">2</span>) + self.eps)<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<p>这里有个坑，就是会有一个内存的检测，要求不能出现多余的tensor</p>
<p>也就是说，除了构建计算图之后，能用原生的data就用。我之前在<code>BatchNorm1d</code>中计算<code>self.running_mean</code>时就直接用的Tensor，造成了内存的问题。</p>
<h1 id="Question-4"><a href="#Question-4" class="headerlink" title="Question 4"></a>Question 4</h1><h2 id="Transformations"><a href="#Transformations" class="headerlink" title="Transformations"></a>Transformations</h2><h3 id="RandomFlipHorizontal"><a href="#RandomFlipHorizontal" class="headerlink" title="RandomFlipHorizontal"></a>RandomFlipHorizontal</h3><p>将数组的第二个维度翻转即可，<del>具体为啥应该不是重点</del></p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs c++">flip_img = np.random.<span class="hljs-built_in">rand</span>() &lt; self.p<br># BEGIN YOUR SOLUTION<br><span class="hljs-keyword">if</span> flip_img:<br>    <span class="hljs-keyword">return</span> img[:, ::<span class="hljs-number">-1</span>, :]<br><span class="hljs-keyword">else</span>:<br>    <span class="hljs-keyword">return</span> img<br></code></pre></td></tr></table></figure>

<h3 id="RandomCrop"><a href="#RandomCrop" class="headerlink" title="RandomCrop"></a>RandomCrop</h3><p>随机在上下左右四个方向移动图片，用零填充</p>
<p>实现思路是先在四个方向填充很多0，再根据随机生成的偏移来对数组切片即可</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs c++">shift_x, shift_y = np.random.<span class="hljs-built_in">randint</span>(<br>    low=-self.padding, high=self.padding + <span class="hljs-number">1</span>, size=<span class="hljs-number">2</span>)<br># BEGIN YOUR SOLUTION<br>img_pad = np.<span class="hljs-built_in">pad</span>(img, [(self.padding, self.padding),<br>                 (self.padding, self.padding), (<span class="hljs-number">0</span>, <span class="hljs-number">0</span>)])<br>h, w, _ = img_pad.shape<br><span class="hljs-keyword">return</span> img_pad[self.padding + shift_x:h - self.padding + shift_x, self.padding + shift_y:w - self.padding + shift_y, :]<br># END YOUR SOLUTION<br><br></code></pre></td></tr></table></figure>

<h2 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h2><p>dataset类要完成的任务是根据index给出数据，其中index可能是整数，也可能是其他类型的索引，比如切片，比如列表(Python内置的列表不支持用列表访问列表，但是numpy的数组是支持的，而我们这里的images是用numpy的数组存储的，所以需要考虑到)。因此，<code>self.images[index]</code>可能是一个列表，其中有多个image</p>
<p>同时，我们这里可能需要进行transforms的操作，Dataset类已经给出了<code>apply_transforms</code>函数，我们只需要调用即可。不过调用的时候要注意，我们之前实现的各种transform的操作都是在(H,W,C)中的H和W上进行的，所以将数据放入<code>apply_transforms</code>之前，要将数据reshape成<code>(28, 28, 1)</code>，同时，在操作结束之后，将图片回复正常形状</p>
<p>最后，返回数据和标签组成的元组</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">parse_mnist</span><span class="hljs-params">(image_filename, label_filename)</span>:</span><br><span class="hljs-function">    # BEGIN YOUR CODE</span><br><span class="hljs-function">    with gzip.open(image_filename, <span class="hljs-string">&#x27;rb&#x27;</span>) as f:</span><br><span class="hljs-function">        file_content =</span> f.<span class="hljs-built_in">read</span>()<br>        # &gt;I 代表大端无符号整数<br>        num = <span class="hljs-keyword">struct</span>.<span class="hljs-built_in">unpack</span>(<span class="hljs-string">&#x27;&gt;I&#x27;</span>, file_content[<span class="hljs-number">4</span>:<span class="hljs-number">8</span>])[<span class="hljs-number">0</span>]<br>        # 第一个参数代表要读出的格式<br>        # 第二个参数代表要读的东西<br>        X = np.<span class="hljs-built_in">array</span>(<span class="hljs-keyword">struct</span>.<span class="hljs-built_in">unpack</span>(<br>            <span class="hljs-string">&#x27;B&#x27;</span> * <span class="hljs-number">784</span> * num, file_content[<span class="hljs-number">16</span>:<span class="hljs-number">16</span> + <span class="hljs-number">784</span> * num]<br>            ), dtype=np.float32)<br>        X.<span class="hljs-built_in">resize</span>((num, <span class="hljs-number">784</span>))<br>    with gzip.<span class="hljs-built_in">open</span>(label_filename, <span class="hljs-string">&#x27;rb&#x27;</span>) as f:<br>        file_content = f.<span class="hljs-built_in">read</span>()<br>        num = <span class="hljs-keyword">struct</span>.<span class="hljs-built_in">unpack</span>(<span class="hljs-string">&#x27;&gt;I&#x27;</span>, file_content[<span class="hljs-number">4</span>: <span class="hljs-number">8</span>])[<span class="hljs-number">0</span>]<br>        y = np.<span class="hljs-built_in">array</span>([<span class="hljs-keyword">struct</span>.<span class="hljs-built_in">unpack</span>(<span class="hljs-string">&#x27;B&#x27;</span>, file_content[<span class="hljs-number">8</span> + i:<span class="hljs-number">9</span> + i])[<span class="hljs-number">0</span>]<br>                     <span class="hljs-keyword">for</span> i in <span class="hljs-built_in">range</span>(num)], dtype=np.uint8)<br><br>    X = X / <span class="hljs-number">255.0</span><br>    <span class="hljs-keyword">return</span> X, y<br>    # END YOUR CODE<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-built_in">MNISTDataset</span>(Dataset):<br>    def __init__(<br>            self,<br>            image_filename: str,<br>            label_filename: str,<br>            transforms: Optional[List] = None,<br>            ):<br>        # BEGIN YOUR SOLUTION<br>        <span class="hljs-built_in">super</span>().__init__(transforms)<br>        self.images, self.labels = <span class="hljs-built_in">parse_mnist</span>(<br>            image_filename=image_filename,<br>            label_filename=label_filename<br>            )<br>        # END YOUR SOLUTION<br><br>    def __getitem__(self, index) -&gt; object:<br>        img = self.images[index]<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(img.shape) &gt; <span class="hljs-number">1</span>:<br>            img = np.<span class="hljs-built_in">array</span>([self.<span class="hljs-built_in">apply_transforms</span>(<br>                i.<span class="hljs-built_in">reshape</span>(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)).<span class="hljs-built_in">reshape</span>(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>) <span class="hljs-keyword">for</span> i in img])<br>        <span class="hljs-keyword">else</span>:<br>            img = self.<span class="hljs-built_in">apply_transforms</span>(<br>                img.<span class="hljs-built_in">reshape</span>(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>)).<span class="hljs-built_in">reshape</span>(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>)<br>        label = self.labels[index]<br>        <span class="hljs-built_in">return</span> (img, label)<br><br>    def __len__(self) -&gt; <span class="hljs-type">int</span>:<br>        # BEGIN YOUR SOLUTION<br>        <span class="hljs-keyword">return</span> self.labels.shape[<span class="hljs-number">0</span>]<br>        # END YOUR SOLUTION<br><br></code></pre></td></tr></table></figure>

<p>这里参考大佬的代码换了个解析图片文件的函数，解析一个图片的思路如下</p>
<ol>
<li>首先得清楚这个图片文件的格式，通常在文件开始处有一些元信息，真正读取数据时已经跳过</li>
<li>首先，打开文件并读入文件的所有内容</li>
<li>读取文件的元信息，比如这个文件的数据有多长，或者文件包含多少个图片。可以通过<code>struct.unpack</code>函数完成这个操作，这个函数的第一个参数是读取数据的格式，第二个参数是需要读取的文件的区域</li>
<li>读取文件的数据，按自己想要的格式存储起来</li>
<li>对图片数据进行归一化（需要的话）</li>
</ol>
<p>感觉读图片文件这种操作，有点繁琐且没意思，知道思路之后<del>能用现成的就用现成的吧</del></p>
<h2 id="Dataloader"><a href="#Dataloader" class="headerlink" title="Dataloader"></a>Dataloader</h2><p>Dataloader这玩意提供了一层抽象，可以实现如下附加功能</p>
<ol>
<li>以batch的大小读取数据</li>
<li>决定是否要打乱</li>
</ol>
<p>其中以batch的大小读取数据可以轻松实现，因为在dataset中已经支持批量读取数据了</p>
<p>而打乱的操作可以用如下思路实现</p>
<ol>
<li>假设现在有n个数据</li>
<li>生成0到n-1的全排列</li>
<li>以batch为大小将全排列进行划分，得到m个列表</li>
<li>这m个列表就是m个batch，直接用列表去访问dataset即可，numpy数组可以正确处理这里访问方式</li>
</ol>
<p>最后，通过dataloader返回的数据和标签，都是tensor类型，因为直接放到神经网络中去训练了，这里总结一下dataloader返回的数据的格式，它返回的是一个元组，元组的第一个是数据，第二个标签</p>
<ol>
<li>数据的格式$(m,H,W,C)$，所以通过下标i就可以拿到第i个数据</li>
<li>标签的格式是$(m,)$，通过下标i就可以拿到第i个标签</li>
</ol>
<p>这里还涉及到了一些Python的语法，以及运算符的重载</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-keyword">class</span> <span class="hljs-title class_">DataLoader</span>:<br>    dataset: Dataset<br>    batch_size: Optional[<span class="hljs-type">int</span>]<br><br>    def __init__(<br>            self,<br>            dataset: Dataset,<br>            batch_size: Optional[<span class="hljs-type">int</span>] = <span class="hljs-number">1</span>,<br>            shuffle: <span class="hljs-type">bool</span> = False,<br>            ):<br><br>        self.dataset = dataset<br>        self.shuffle = shuffle<br>        self.batch_size = batch_size<br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> self.shuffle:<br>            self.ordering = np.<span class="hljs-built_in">array_split</span>(np.<span class="hljs-built_in">arange</span>(<span class="hljs-built_in">len</span>(dataset)),<br>                                           <span class="hljs-built_in">range</span>(batch_size, <span class="hljs-built_in">len</span>(dataset), batch_size))<br><br>    def __iter__(self):<br>        # BEGIN YOUR SOLUTION<br>        self.index = <span class="hljs-number">-1</span><br>        <span class="hljs-keyword">if</span> self.shuffle:<br>            self.ordering = np.<span class="hljs-built_in">array_split</span>(np.random.<span class="hljs-built_in">permutation</span>(<span class="hljs-built_in">len</span>(self.dataset)),<br>                                           <span class="hljs-built_in">range</span>(self.batch_size, <span class="hljs-built_in">len</span>(self.dataset), self.batch_size))<br>        # END YOUR SOLUTION<br>        <span class="hljs-keyword">return</span> self<br><br>    def __next__(self):<br>        # BEGIN YOUR SOLUTION<br>        self.index += <span class="hljs-number">1</span><br>        <span class="hljs-keyword">if</span> self.index &gt;= <span class="hljs-built_in">len</span>(self.ordering):<br>            raise StopIteration<br>        samples = self.dataset[self.ordering[self.index]]<br>        <span class="hljs-keyword">return</span> <span class="hljs-built_in">Tensor</span>(samples[<span class="hljs-number">0</span>]), <span class="hljs-built_in">Tensor</span>(samples[<span class="hljs-number">1</span>])<br>        # END YOUR SOLUTION<br><br></code></pre></td></tr></table></figure>

<h1 id="Question-5"><a href="#Question-5" class="headerlink" title="Question 5"></a>Question 5</h1><h2 id="ResidualBlock"><a href="#ResidualBlock" class="headerlink" title="ResidualBlock"></a>ResidualBlock</h2><p>直接照着图翻译即可，先构造出残差网络的主函数，然后再构建残差网络，最后加上一个relu</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">ResidualBlock</span><span class="hljs-params">(dim, hidden_dim, norm=nn.BatchNorm1d, drop_prob=<span class="hljs-number">0.1</span>)</span>:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    main_path =</span> nn.<span class="hljs-built_in">Sequential</span>(nn.<span class="hljs-built_in">Linear</span>(dim, hidden_dim), <span class="hljs-built_in">norm</span>(hidden_dim), nn.<span class="hljs-built_in">ReLU</span>(<br>        ), nn.<span class="hljs-built_in">Dropout</span>(drop_prob), nn.<span class="hljs-built_in">Linear</span>(hidden_dim, dim), <span class="hljs-built_in">norm</span>(dim))<br>    res = nn.<span class="hljs-built_in">Residual</span>(main_path)<br>    <span class="hljs-keyword">return</span> nn.<span class="hljs-built_in">Sequential</span>(res, nn.<span class="hljs-built_in">ReLU</span>())<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<h2 id="MLPResNet"><a href="#MLPResNet" class="headerlink" title="MLPResNet"></a>MLPResNet</h2><p>继续翻译图</p>
<p>不过这里有个玄学，那numblocks个残差网络，如果我先定义好，然后在拿到这里面来，最后就会报错，或者我在这个函数里多定义其他的变量，但是不放到res中，也会报错，明明新定义的模块都没有使用，目前还不清楚为啥</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">MLPResNet</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">        dim,</span></span><br><span class="hljs-params"><span class="hljs-function">        hidden_dim=<span class="hljs-number">100</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">        num_blocks=<span class="hljs-number">3</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">        num_classes=<span class="hljs-number">10</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">        norm=nn.BatchNorm1d,</span></span><br><span class="hljs-params"><span class="hljs-function">        drop_prob=<span class="hljs-number">0.1</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">        )</span>:</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    res =</span> nn.<span class="hljs-built_in">Sequential</span>(nn.<span class="hljs-built_in">Linear</span>(dim, hidden_dim), nn.<span class="hljs-built_in">ReLU</span>(),<br>                        *[<span class="hljs-built_in">ResidualBlock</span>(dim=hidden_dim, hidden_dim=hidden_dim <span class="hljs-comment">// 2,</span><br>                                        norm=norm, drop_prob=drop_prob) <span class="hljs-keyword">for</span> _ in <span class="hljs-built_in">range</span>(num_blocks)],<br>                        nn.<span class="hljs-built_in">Linear</span>(hidden_dim, num_classes))<br><br>    <span class="hljs-keyword">return</span> res<br></code></pre></td></tr></table></figure>

<h2 id="epoch"><a href="#epoch" class="headerlink" title="epoch"></a>epoch</h2><p>epoch就是一次迭代需要做的事情，那就需要使用模型，损失函数，优化器</p>
<p>如果现在是测试模式，那连优化器都不用了，只需要使用模型得到输出，计算以下损失和准确率即可</p>
<p>这里我们根据opt判断是训练或者还是测试模式，可以通过<code>model.eval</code>或者<code>model.train</code>设置这些模式，本质上在这两个函数中，就是将这模块的所有组成部分的training属性设置为false或者true。通过traning可以控制一些模块在forward时的表现，比如那个batch-norm</p>
<p>要注意的是，这个模式和是否需要进行梯度下降并不等价，这应该是单纯控制forward时的作用</p>
<p>在计算损失的时候，不能直接用tensor来计算了，要用tensor里真实的data</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">epoch</span><span class="hljs-params">(dataloader, model, opt=None)</span>:</span><br><span class="hljs-function">    np.random.seed(<span class="hljs-number">4</span>)</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    tot_loss, tot_err =</span> [], <span class="hljs-number">0.0</span><br>    loss_fc = nn.<span class="hljs-built_in">SoftmaxLoss</span>()<br>    <span class="hljs-keyword">if</span> opt is None:<br>        model.<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-keyword">for</span> X, y in dataloader:<br>            logit = <span class="hljs-built_in">model</span>(X)<br>            tot_loss.<span class="hljs-built_in">append</span>(<span class="hljs-built_in">loss_fc</span>(logit, y).<span class="hljs-built_in">numpy</span>())<br>            tot_err += np.<span class="hljs-built_in">sum</span>(logit.<span class="hljs-built_in">numpy</span>().<span class="hljs-built_in">argmax</span>(axis=<span class="hljs-number">1</span>) != y.<span class="hljs-built_in">numpy</span>())<br>    <span class="hljs-keyword">else</span>:<br>        model.<span class="hljs-built_in">train</span>()<br>        <span class="hljs-keyword">for</span> X, y in dataloader:<br>            logit = <span class="hljs-built_in">model</span>(X)<br>            loss = <span class="hljs-built_in">loss_fc</span>(logit, y)<br>            tot_loss.<span class="hljs-built_in">append</span>(loss.<span class="hljs-built_in">numpy</span>())<br>            tot_err += np.<span class="hljs-built_in">sum</span>(logit.<span class="hljs-built_in">numpy</span>().<span class="hljs-built_in">argmax</span>(axis=<span class="hljs-number">1</span>) != y.<span class="hljs-built_in">numpy</span>())<br>            opt.<span class="hljs-built_in">reset_grad</span>()<br>            loss.<span class="hljs-built_in">backward</span>()<br>            opt.<span class="hljs-built_in">step</span>()<br>    <span class="hljs-keyword">return</span> tot_err / <span class="hljs-built_in">len</span>(dataloader.dataset), np.<span class="hljs-built_in">mean</span>(tot_loss)<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>

<h2 id="Train-Mnist"><a href="#Train-Mnist" class="headerlink" title="Train Mnist"></a>Train Mnist</h2><p>这其实就是最终用户使用的接口，自己定义模型，优化器，损失函数，得到数据集</p>
<p>其中有个细节就是在定义优化器的时候，我们传入了模型的所有需要更新的参数，这才是决定梯度下降时到底要更新哪些参数的位置</p>
<p>还有个有意思的点，在使用l2惩罚项的时候，看起来是在损失函数上加上了这个惩罚项，而在真正实现的时候，是在优化器中定义weight_decay，这应该也是为了节省资源的一种做法，可以省去很多不必要的计算</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><code class="hljs c++"><span class="hljs-function">def <span class="hljs-title">train_mnist</span><span class="hljs-params">(</span></span><br><span class="hljs-params"><span class="hljs-function">        batch_size=<span class="hljs-number">100</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">        epochs=<span class="hljs-number">10</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">        optimizer=ndl.optim.Adam,</span></span><br><span class="hljs-params"><span class="hljs-function">        lr=<span class="hljs-number">0.001</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">        weight_decay=<span class="hljs-number">0.001</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">        hidden_dim=<span class="hljs-number">100</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">        data_dir=<span class="hljs-string">&quot;data&quot;</span>,</span></span><br><span class="hljs-params"><span class="hljs-function">        )</span>:</span><br><span class="hljs-function">    np.random.seed(<span class="hljs-number">4</span>)</span><br><span class="hljs-function">    # BEGIN YOUR SOLUTION</span><br><span class="hljs-function">    resnet =</span> <span class="hljs-built_in">MLPResNet</span>(<span class="hljs-number">28</span> * <span class="hljs-number">28</span>, hidden_dim=hidden_dim, num_classes=<span class="hljs-number">10</span>)<br>    opt = <span class="hljs-built_in">optimizer</span>(resnet.<span class="hljs-built_in">parameters</span>(), lr=lr, weight_decay=weight_decay)<br>    train_set = <span class="hljs-built_in">MNISTDataset</span>(<br>        f<span class="hljs-string">&quot;&#123;data_dir&#125;/train-images-idx3-ubyte.gz&quot;</span>, f<span class="hljs-string">&quot;&#123;data_dir&#125;/train-labels-idx1-ubyte.gz&quot;</span>)<br>    test_set = <span class="hljs-built_in">MNISTDataset</span>(<br>        f<span class="hljs-string">&quot;&#123;data_dir&#125;/t10k-images-idx3-ubyte.gz&quot;</span>, f<span class="hljs-string">&quot;&#123;data_dir&#125;/t10k-labels-idx1-ubyte.gz&quot;</span>)<br>    train_loader = <span class="hljs-built_in">DataLoader</span>(train_set, batch_size=batch_size, shuffle=True)<br>    test_loader = <span class="hljs-built_in">DataLoader</span>(test_set, batch_size=batch_size)<br>    <span class="hljs-keyword">for</span> _ in <span class="hljs-built_in">range</span>(epochs):<br>        train_err, train_loss = <span class="hljs-built_in">epoch</span>(train_loader, resnet, opt=opt)<br>    test_err, test_loss = <span class="hljs-built_in">epoch</span>(test_loader, resnet, None)<br>    <span class="hljs-keyword">return</span> train_err, train_loss, test_err, test_loss<br>    # END YOUR SOLUTION<br></code></pre></td></tr></table></figure>
                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
    <div class="post-meta mr-3 d-flex align-items-center">
      <i class="iconfont icon-category"></i>
      

<span class="category-chains">
  
  
    
      <span class="category-chain">
        
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" class="category-chain-item">深度学习框架</a>
  
  
    <span>></span>
    
  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/CMU10414/" class="category-chain-item">CMU10414</a>
  
  

  

      </span>
    
  
</span>

    </div>
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6/" class="print-no-link">#深度学习框架</a>
      
        <a href="/tags/CMU10414/" class="print-no-link">#CMU10414</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>cmu10414_lab2</div>
      <div>http://example.com/2023/11/02/cmu10414-lab2/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>作者</div>
          <div>Cao Dong</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>发布于</div>
          <div>2023年11月2日</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>许可协议</div>
          <div>
            
              
              
                <a class="print-no-link" target="_blank" href="https://creativecommons.org/licenses/by/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - 署名">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2023/11/08/cmu10414-lab3/" title="cmu10414_lab3">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">cmu10414_lab3</span>
                        <span class="visible-mobile">上一篇</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2023/10/29/cmu10414-lab1/" title="cmu10414_lab1">
                        <span class="hidden-mobile">cmu10414_lab1</span>
                        <span class="visible-mobile">下一篇</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header">
    <i class="iconfont icon-list"></i>
    <span>目录</span>
  </p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  







    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">搜索</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">关键词</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://hexo.io" target="_blank" rel="nofollow noopener"><span>Hexo</span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Fluid</span></a> 
    </div>
  
  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.4/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    <script  src="/js/img-lazyload.js" ></script>
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.20.1/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>





<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">博客在允许 JavaScript 运行的环境下浏览效果更佳</div>
  </noscript>
</body>
</html>
